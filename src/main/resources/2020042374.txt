As Mark Twain famously popularized in the public consciousness, “There are three kinds of lies: lies, damned lies, and statistics.” Whether through malice, poor training or simple ignorance, “bad statistics” has a rich and storied legacy stretching back as long as humans have been counting things. Countless books, papers and blogs chronicle the myriad ways in which data and statistics are abused to lend false support to arguments in fields ranging from the academic world to public policy. As data-driven journalism is on the rise and calls continue to grow for increased evidence-based “fact checking,” it is worth stepping back to ask how much of the “fake news” that surrounds us today is based at least in part on bad statistics.

Not a day goes by without a flurry of data-driven memes passing through my Facebook news feed, sailing by on my Twitter stream or landing as alerts in my email inbox that cite what appear to be reputable datasets and using them to offer surprising conclusions, typically wrapped up in a mesmerizing infographic. Yet, when I pick any of these memes at random and delve into it, I find that it is the rare meme indeed that stands up to statistical scrutiny.

Some memes I come across are easy to discard as outright fabrications, citing non-existent datasets, non-existent authors, non-existent journals or citing real (typically very prominent) researchers and institutes in the field, but who when contacted say they’ve never heard of the research they are claimed to be the author of. Textual memes are the most common in this category, since it requires so little effort to send out a tweet along the lines of “A recent Gallup poll states that 80% of Americans believe that climate change is false.” Such memes can be made to look more authoritative by whipping up a quick graph in Excel. For such visual memes, sometimes just right-clicking on the graph in the Google Chrome browser and selecting “Search Google for image” will turn up fact checking sites or academic blogs who have researched the graph and confirmed it to be a fabrication.

Today In: Tech

I’ve even seen a few memes that have taken a legitimate “science-y looking” graph from a paper in one field and use it as an illustration for a claim in a different field. Just recently I saw a meme go by in my Facebook feed that featured a graph of an exponential curve with all sorts of statistical measures in the background that was used to illustrate a claim about global warming trends over the past 50 years. The odd part is that X and Y axes were cut off and some of the annotations on the graph related to the medical field. In fact, after a bit of searching I was able to find that the author of the meme had apparently just grabbed a nice exponential-looking graph from a completely unrelated medical paper (perhaps found via a quick Google Scholar search).

The rise of preprints, postprints and academic publishing through blogs has had a dangerous effect on scientific trust, accustoming the general public to seeing a news article discussing a new scientific advance that links to a preprint of the article on the faculty member’s personal blog, rather than on the journal’s website. This means that when a member of the public sees a meme that cites an academic paper supposedly published in the latest issue of Nature, but the link goes to a PDF on a random website that purports to be a Harvard professor’s personal blog, many readers won’t blink an eye and simply trust that the paper really is a preprint of a new Nature article by a Harvard professor.

PROMOTED

Muddying the waters even further, the rise of predatory publishers and fly-by-night journals means that a meme could link to a paper on an actual professional-looking journal website with a prestigious-sounding name and listing many prominent faculty on its editorial board (who may not even be aware their names are being used). Peer review standards are often essentially non-existent at such journals, meaning nearly any submission is accepted.

It thus takes little more than a quick Google search these days to locate an academic paper published in a prestigious-sounding journal that makes any argument you want and claims to have the data, statistics and citations to support that argument rigorously and without question. To the average member of the public, "peer review" is an unknown concept and a paper published in Nature is not any more reputable than one published in The Journal Of Prestigious And World Changing Research.

However, the greatest single contributor to data-driven “fake news” are the myriad statistical fallacies that so easily befall even academics in fields that do not emphasize rigorous statistical training (though even stats-heavy fields are not immune to statistical arguments). Beyond the obvious candidates like suggestions of correlation implying causation and improper use of statistical techniques, perhaps one of the greatest enablers of fake news in the memes I come across is sampling bias and selective definitions.

As but one example, definitions of what precisely constitutes a “terror attack” are notoriously controversial. Was something a “mass shooting,” a “terrorist attack,” or an “act of mental illness?” I recently saw one meme that argued that there had never been another act of terrorism on US soil since 9/11 because all subsequent US attacks were the result of mentally ill individuals, rather than terrorism. Another recent meme I saw claimed that no American had been injured or killed by a foreign-born attacker on US soil and only in tiny print in a small footnote was there a statement limiting the time frame of analysis so as to not include the 9/11 attacks, the San Bernardino attack and other cases. One national poll I saw during the presidential campaign season made bold claims about national support for Clinton, but in its methodology revealed that more than 80% of its sample size were Democrats and Independents. This raises the critical question – would we label these as “fake news,” as “factually accurate but misleading” or as “absolutely true?”

Therein lies one of the great challenges of the “fake news” debate – many of the data-driven memes (and news articles) swirling about are, on purely technical merits, factually accurate based on the carefully-constructed population sample they use. The question is whether something that is factually accurate can also be labeled as “fake news” when it comes to misleading the public, given that the results of even the best-run experiments are all too quickly separated from the myriad caveats that temper those conclusions. A surprising poll that clearly indicates an overwhelming sampling bias towards Democrats is eventually transformed into a headline devoid of any mention of partisan skew. A claim that there has never been a terror attack on US soil since 9/11 spreads through social media and sheds its footnote clarifying that it refers to only a small portion of that 15-year period.

How do we handle statistical fallacies in a world in which few citizens (and even academics) have even a basic understanding of statistics or data? Even more troubling, how do we handle factually true statements that utilize such a carefully constructed population sample that their argument is almost meaningless? They can’t technically be flagged as “fake news” since they are factually correct, but it is also likely that as they spread those footnotes will be lost. If a factoid is shared without its original caveats does that then make it false? If a meme simply states “There has never been a terror attack on US soil since 9/11” and the footnotes clarifying the time periods and definition of “terror attack” it refers to have long since been lost, does that make the meme false or is the meme still true since it is factually correct under the specific assumptions and population construction used by its original author?

These are fascinating questions as we confront the duality of vastly increased access to data and a data-illiterate population that lacks the statistical training to understand how to properly use that data to draw conclusions. Adding to this volatile mix, social media ensures that even the most skewed factoid can be extracted from a dataset and go viral, quickly losing connection to the myriad definitional caveats that enabled it to cling to truthfulness.

Even when using simple techniques like counts over time, issues like data normalization and the unique nuances of dataset construction are particularly perplexing even to those with deep statistical backgrounds, meaning that even seasoned data journalists regularly publish findings that are deeply flawed and lead to further false and misleading headlines and interpretations.

Putting this all together, as I argued in December, we cannot begin to fight fake news until we focus on increasing society’s data and information literacy.